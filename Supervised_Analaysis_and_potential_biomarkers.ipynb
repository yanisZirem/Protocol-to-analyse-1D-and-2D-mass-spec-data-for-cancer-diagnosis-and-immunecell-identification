{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0096d115",
   "metadata": {},
   "source": [
    "# Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c257adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "import importlib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "import joblib\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the data\n",
    "data = pd.read_csv(\"Data_Train_Toy.csv\")  # Import the CSV file\n",
    "print(\"Labels:\", data.Class.unique())  # Print unique labels\n",
    "print(\"Number of samples:\", data.Class.value_counts())  # Print number of samples per class\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot average spectra for each class\n",
    "def plot_average_spectra(data, class_column='Class', threshold=None, colors=None):\n",
    "    fig = go.Figure()\n",
    "    unique_classes = data[class_column].unique()\n",
    "    if colors is None:\n",
    "        colors = {class_label: f'rgb({i * 10}, {255 - i * 40}, {i * 20})'\n",
    "                  for i, class_label in enumerate(unique_classes)}\n",
    "    for class_label in unique_classes:\n",
    "        class_data = data[data[class_column] == class_label].drop(class_column, axis=1)\n",
    "        mean_spectrum = class_data.mean()\n",
    "        fig.add_trace(go.Scatter(x=mean_spectrum.index, y=mean_spectrum.values,\n",
    "                                 mode='lines', name=f'Class {class_label}',\n",
    "                                 line=dict(color=colors.get(class_label, 'blue'))))\n",
    "    fig.update_layout(width=1000, xaxis_title='m/z', yaxis_title='Relative Intensities')\n",
    "    fig.update_xaxes(tickangle=45, tickfont=dict(size=10))  \n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom colors for the classes\n",
    "custom_colors = {'Tumor':'red','Necrosis':'black','Benign':'green'}\n",
    "\n",
    "# Plot the average spectra\n",
    "plot = plot_average_spectra(data, class_column='Class', threshold=None, colors=custom_colors)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf07ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a specific sample spectrum\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data.columns, y=data.iloc[20].values, mode='lines', line=dict(color='orange')))\n",
    "fig.update_layout(width=1000, xaxis_title='m/z', yaxis_title='relative intensities', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different ML models\n",
    "y = data.pop('Class')\n",
    "X = data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find and build the best model based on F1 score\n",
    "def find_and_build_best_model(models, X_train, y_train, specific_model=None):\n",
    "    best_model_name = None\n",
    "    best_f1_score = -1\n",
    "    \n",
    "    for model_name in models.index:\n",
    "        f1_score = models.at[model_name, 'F1 Score']\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            best_model_name = model_name\n",
    "    if best_model_name:\n",
    "        print(\"Best Classifier:\", best_model_name)\n",
    "        \n",
    "        try:\n",
    "            if best_model_name == 'RidgeClassifier':\n",
    "                best_model = RidgeClassifier()\n",
    "            else:\n",
    "                model_module = importlib.import_module('sklearn.linear_model')\n",
    "                if hasattr(model_module, best_model_name):\n",
    "                    best_model = getattr(model_module, best_model_name)()\n",
    "                else:\n",
    "                    model_module = importlib.import_module('sklearn.ensemble')\n",
    "                    if hasattr(model_module, best_model_name):\n",
    "                        best_model = getattr(model_module, best_model_name)()\n",
    "                    else:\n",
    "                        model_module = importlib.import_module('sklearn.svm')\n",
    "                        if hasattr(model_module, best_model_name):\n",
    "                            best_model = getattr(model_module, best_model_name)()\n",
    "                        else:\n",
    "                            if best_model_name.startswith(\"LGBM\"):\n",
    "                                best_model = getattr(lgb, best_model_name)()\n",
    "                            elif best_model_name.startswith(\"XGB\"):\n",
    "                                best_model = getattr(xgb, best_model_name)()\n",
    "                            else:\n",
    "                                print(\"Best Classifier not found.\")\n",
    "                                return None, None\n",
    "            \n",
    "            pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            return best_model_name, pipeline\n",
    "        except ImportError:\n",
    "            print(\"Best Classifier not found.\")\n",
    "            return None, None\n",
    "    else:\n",
    "        print(\"Best Classifier not found.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and build the best model\n",
    "best_model_name, best_model_pipeline = find_and_build_best_model(models, X_train, y_train, specific_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217734c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display confusion matrix, scores, and classification report\n",
    "def confusion_matrix_scores_classification_report(pipeline, X_test, y_test):\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print('Accuracy:', score)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ConfusionMatrixDisplay.from_estimator(pipeline, X_test, y_test)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cross-validation and reporting results\n",
    "def cross_validate_and_report(pipeline, X, y):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    cv_scores = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "    print('CV Scores:', cv_scores)\n",
    "    print('Mean CV Score:', cv_scores.mean())\n",
    "    print('Std CV Score:', cv_scores.std())\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=kfold)\n",
    "    print(classification_report(y, y_pred))\n",
    "    class_names = pipeline.named_steps[pipeline.steps[-1][0]].classes_\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap='viridis')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=class_names, yticklabels=class_names, title='Confusion matrix', ylabel='True label', xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha=\"center\", va=\"center\", color=\"black\" if cm[i, j] > thresh else \"yellow\")\n",
    "    fig.tight_layout()\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aee0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix and classification report for validation data\n",
    "confusion_matrix_scores_classification_report(best_model_pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation and report results\n",
    "cross_validate_and_report(best_model_pipeline, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(best_model_pipeline, \"X_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate the model on new data\n",
    "val = pd.read_csv(\"data_test_toy.csv\")\n",
    "val_id = val\n",
    "val = val.drop([\"Class\"], axis=1)\n",
    "validation = best_model_pipeline.predict(val)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8768220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare predicted and true labels\n",
    "df = pd.DataFrame(validation)\n",
    "df[\"True Labels\"] = val_id[\"Class\"]\n",
    "df = df.rename(columns={0: \"Predicted Labels\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe55b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix and classification report for validation data\n",
    "confusion_matrix_scores_classification_report(best_model_pipeline, val, val_id[\"Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1364b1",
   "metadata": {},
   "source": [
    "# Predictions explanation and potential biomarekrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get feature importance using eli5 (LIME)\n",
    "def eli5_feature_importance(pipeline, X_train, top_features=40):\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contribution = eli5.show_weights(model, feature_names=X_train.columns.tolist(), top=top_features, feature_re='^.*$')\n",
    "    return sample_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "sample_contribution = eli5_feature_importance(best_model_pipeline, X_train)\n",
    "sample_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save feature contributions to CSV\n",
    "def save_contributions(csv_name, pipeline, X_train):\n",
    "    model = pipeline.named_steps[pipeline.steps[-1][0]]\n",
    "    sample_contributions = []\n",
    "    for idx in range(len(X_train.index)):\n",
    "        sample_contribution_df = eli5.explain_weights_df(model, feature_names=X_train.columns.tolist(), feature_re='^.*$')\n",
    "        sample_contributions.append(sample_contribution_df)\n",
    "    all_contributions_df = pd.concat(sample_contributions)\n",
    "    all_contributions_df.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature contributions\n",
    "save_contributions(\"X_contributions.csv\", best_model_pipeline, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
